# çº¿æ€§å›å½’

## çº¿æ€§å›å½’

æˆ‘ä»¬é€šè¿‡çº¿æ€§å›å½’ç®—æ³•æ‰¾å‡ºè‡ªå˜é‡å’Œå› å˜é‡é—´çš„æœ€ä½³çº¿æ€§å…³ç³»ï¼Œå›¾å½¢ä¸Šå¯ä»¥ç¡®å®šä¸€æ¡æœ€ä½³ç›´çº¿ã€‚

è¿™æ¡æœ€ä½³ç›´çº¿å°±æ˜¯å›å½’çº¿ã€‚è¿™ä¸ªå›å½’å…³ç³»å¯ä»¥ç”¨ğ‘Œ=ğ‘ğ‘‹+ğ‘*Y*=*aX*+*b*è¡¨ç¤ºã€‚

## ä¸€èˆ¬ä½¿ç”¨å‡æ–¹è¯¯å·®è®¡ç®—æŸå¤±å‡½æ•°

```python
import numpy as np
from matplotlib import pyplot as plt

class Line:
    def __init__(self, data):
        self.w = 1
        self.b = 0
        self.learning_rate = 0.01ï¼ˆå°±æ˜¯æ¯æ¬¡è¶‹è¿‘ä¸æœ€ä½³wçš„æ­¥é•¿ï¼‰
        self.fig, (self.ax1, self.ax2) = plt.subplots(2, 1)
        self.loss_list = []

    def get_data(self, data):
        self.X = np.array(data)[:, 0]
        self.y = np.array(data)[:, 1]
		#sigmoidå‡½æ•°
    def predict(self, x):
        return self.w * x + self.b
    
    def train(self, epoch_times):
        for epoch in range(epoch_times):
            total_loss = 0
            for x, y in zip(self.X, self.y):
                y_pred = self.predict(x)
                # è®¡ç®—å¯¼æ•°ï¼ˆå¯¼æ•°wçš„**ç¬¦å·**å³ä¸ºæ–¹å‘ï¼Œ**æ•°å€¼æ­£çš„å°±å¤ªè¿œäº†ï¼Œè´Ÿçš„å°±å¤ªè¿‘äº†**ï¼‰
                gradient_w = -2 * x * (y - y_pred)
                gradient_b = -2 * (y - y_pred)
                # æ›´æ–°wï¼Œx
                #æ‰€æ±‚å°±æ˜¯æœ€ä½³çš„wå³(æ–œç‡)ï¼Œåœ¨**è¿™é‡Œæ­£çš„ç›¸å‡ï¼Œè´Ÿçš„ç›¸åŠ **
                self.w -= self.learning_rate * gradient_w
                self.b -= self.learning_rate * gradient_b
                # è®¡ç®—æŸå¤±å‡½æ•°
                loss = (y - y_pred) ** 2
                total_loss += loss
            epoch_loss = total_loss / len(self.X)
            self.loss_list.append(epoch_loss)
            if epoch % 10 == 0:
                print(f"loss: {epoch_loss}")
                self.plot()
        plt.ioff()
        plt.show()
		#ç»˜å›¾
    def plot(self):
        plt.ion()  # Enable interactive mode
        self.ax2.clear()
        self.ax1.clear()
        x = np.linspace(0, 10, 100)
        self.ax1.scatter(self.X, self.y, c="g")
        self.ax1.plot(x, self.predict(x), c="b")
        self.ax2.plot(list(range(len(self.loss_list))), self.loss_list)
        plt.show()
        plt.pause(0.1)

if __name__ == "__main__":  
    # Input data
    data = [(1, 1), (1.8, 2), (2.5, 3), (4.2, 4), (5, 5), (6, 6), (7, 7)]
    s = Line(data)
    s.get_data(data)
    s.train(100)
```

## ä½¿ç”¨sklearnæ¨¡å—

```python
import numpy as np
from sklearn.linear_model import LinearRegression

# åˆ›å»ºä¸€äº›ç¤ºä¾‹æ•°æ®
X = np.array([[1], [2], [3], [4], [5]])  # è‡ªå˜é‡
y = np.array([2, 3, 4, 4, 6])  # å› å˜é‡
# åˆ›å»ºçº¿æ€§å›å½’æ¨¡å‹
model = LinearRegression()

# æ‹Ÿåˆæ¨¡å‹
model.fit(X, y)

# æ‰“å°å›å½’ç³»æ•°å’Œæˆªè·
print("å›å½’ç³»æ•° (æ–œç‡):", model.coef_)
print("æˆªè·:", model.intercept_)

# é¢„æµ‹æ–°æ•°æ®ç‚¹
new_data_point = np.array([[6]])  # è¦é¢„æµ‹çš„æ–°æ•°æ®ç‚¹
predicted_value = model.predict(new_data_point)
print("é¢„æµ‹å€¼:", predicted_value)
```

## é¢„æµ‹æˆ¿å±‹ä»·æ ¼

```python
from sklearn import datasets
from sklearn.linear_model import LinearRegression

# .fetch_california_housing() åŠ è½½åŠ åˆ©ç¦å°¼äºšå·ä½æˆ¿æ•°æ®é›†
loaded_data = datasets.fetch_california_housing()
# .data æ•°æ®é›†ä¸­çš„ç‰¹å¾æ•°æ®
data_X = loaded_data.data
# .target æ•°æ®é›†ä¸­çš„æ ‡ç­¾æ•°æ®
data_y = loaded_data.target
# åˆ›å»ºçº¿æ€§å›å½’æ¨¡å‹
model = LinearRegression()
# æ‹Ÿåˆæ¨¡å‹
# .fit() æ–¹æ³•æ¥å—ä¸¤ä¸ªå‚æ•°ï¼šç‰¹å¾æ•°æ®å’Œæ ‡ç­¾æ•°æ®
model.fit(data_X, data_y)

# é¢„æµ‹å‰å››æ‰€æˆ¿å±‹ä»·æ ¼
# .predict() æ–¹æ³•æ¥å—ä¸€ä¸ªå‚æ•°ï¼šç‰¹å¾æ•°æ®
print(model.predict(data_X[:4, :]))
# çœŸå®ä»·æ ¼
print(data_y[:4])
```

## æ•ˆæœè¯„ä¼°

```python
print(model.get_params())# è·å–æ¨¡å‹å‚æ•°
# //{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'positive': False}
print(model.score(data_X, data_y))
# // 0.606232685199805
# è¿™æ„å‘³ç€æ•°æ®é›†ä¸­å› å˜é‡çš„ 60% çš„å˜å¼‚æ€§å·²å¾—åˆ°è€ƒè™‘ï¼Œè€Œå…¶ä½™ 40% çš„å˜å¼‚æ€§ä»æœªå¾—åˆ°è§£é‡Šã€‚
# æ‰“å°å›å½’ç³»æ•°å’Œæˆªè·
print("å›å½’ç³»æ•° (æ–œç‡):", model.coef_)
print("æˆªè·:", model.intercept_)
```